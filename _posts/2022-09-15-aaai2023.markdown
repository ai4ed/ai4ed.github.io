---
layout: post
title:  AAAI2023 Artificial Intelligence for Education
date:   2022-09-15 10:00:00
image:  '/images/aaai2023.jpg'
tags:   [Workshop, AAAI]
featured: true
category: workshops
---

**<span style="color:red">This year, the AI4Edu workshop is co-located with the main AAAI 2023 conference in Washington DC, USA on Feb 13, 2023. We encourage offline participation and will also provide online option. Hotel and travel info can be found [here](https://aaai.org/Conferences/AAAI-23/hotel-and-travel/).</span>**


## UPDATES 

* **Workshop Registration**: Registration is available at [https://aaaiconf.cventevents.com/AAAI23](https://aaaiconf.cventevents.com/AAAI23). To participate in this workshop, the **Workshop Only** registration type is enough. For nontechnical regular registrants, the 1-day early bird and normal price are $250 and $350 respectively, and for students, $150 and $200 respectively. Prices about technical registrants discount and 2-day can be found at [https://aaai.org/Conferences/AAAI-23/registration/](https://aaai.org/Conferences/AAAI-23/registration/). 


## Tentative Workshop Schedule

The AI4Edu workshop is co-located with the main AAAI 2023 conference in Washington DC, USA on **<span style="color:red">Feb 13, 2023</span>**. All the time slots are in EST (New York local time).

* 09:00 - 09:10 Opening
* 09:10 - 10:00 **[Keynote Talk]** TBD, Tom Mitchell, School of Computer Science, Carnegie Mellon University
* 10:00 - 10:50 **[Keynote Talk]** [From Autonomy to Synergy: Envisioning Next Generation Human-AI Partnerships](#from-autonomy-to-synergy-envisioning-next-generation-human-ai-partnerships), Sidney D'Mello, Institute of Cognitive Science and the Department of Computer Science, University of Colorado Boulder.
* 10:50 - 11:00 Break

* 11:00 - 11:15 Summary and Winner Announcement of the AAAI2023 Global Knowledge Tracing Challenge
* 11:15 - 11:30 **[Winning Solution Presentation]** TBD 
* 11:30 - 11:45 **[Winning Solution Presentation]** TBD 
* 11:45 - 12:00 **[Winning Solution Presentation]** TBD 

* 12:00 - 13:15 Lunch

* 13:15 - 13:30 **[Cross-submission Presentation]** Transition-Aware Multi-Activity Knowledge Tracing, Siqian Zhao, Chunpai Wang and Shaghayegh Sahebi.
* 13:30 - 13:45 **[Cross-submission Presentation]** pyKT: A Python Library to Benchmark Deep Learning based Knowledge Tracing Models. Zitao Liu, Qiongqiong Liu, Jiahao Chen, Shuyan Huang, Jiliang Tang, Weiqi Luo. 36th Conference on Neural Information Processing Systems (NeurIPS 2022) Datasets and Benchmarks Track 
* 13:45 - 14:00 **[Cross-submission Presentation]** Evaluating the Explainers: Black-Box Explainable Machine Learning for Student Success Prediction in MOOCs. Vinitra Swamy, Bahar Radmehr, Natasa Krco, Mirko Marras, Tanja Käser. The 15th International Conference on Educational Data Mining (EDM 2022)
* 14:00 - 14:15 **[Cross-submission Presentation]** Meta Transfer Learning for Early Success Prediction in MOOCs. Vinitra Swamy, Mirko Marras, Tanja Käser. The 2022 ACM Conference on Learning at Scale (L@S 2022)
* 14:15 - 14:30 **[Cross-submission Presentation]** Code-DKT: A Code-based Knowledge Tracing Model for Programming Tasks. Yang Shi, Min Chi, Tiffany Barnes, Thomas Price. The 15th International Conference on Educational Data Mining (EDM 2022)

* 14:30 - 14:40 Break

* 14:40 - 14:50 **[Paper Presentation]** Tongwen Huang, Xihua Li and Yi Chao. [Finding Similar Exercises in Retrieval Manner](#finding-similar-exercises-in-retrieval-manner)
    * Camera Ready [PDF](https://drive.google.com/file/d/1GvDYhp3z5FCKyDBG27jKmoPR1tVr5S2O/view?usp=share_link)
* 14:50 - 15:00 **[Paper Presentation]** He Zhu, Xihua Li, Xuemin Zhao, Yunbo Cao and Shan Yu. [TQ-Net: Mixed Contrastive Representation Learning For Heterogeneous Test Questions](#tq-net-mixed-contrastive-representation-learning-for-heterogeneous-test-questions)
    * Camera Ready [PDF](https://drive.google.com/file/d/1FzQZAp3zE1noDxrNOyGQ9abG-bY2sXXg/view?usp=share_link)
* 15:00 - 15:10 **[Paper Presentation]** Fang Nan, Feng Tian, Yaozhi Wang, Qidong Liu, Yanze Wu, Jizhong Zhang, Huan Li, Haiping Zhu, Yuzhe Yao, Heng Zhang, Yaqiang Wu and Qinghua Zheng. [Inferring Actions and Joint Attention From Dual-view Classroom Videos](#inferring-actions-and-joint-attention-from-dual-view-classroom-videos)
    * Camera Ready [PDF](https://drive.google.com/file/d/1cfepfbcLpONmgGp9MwvBYr9G9Vp0rIcy/view?usp=share_link)
* 15:10 - 15:20 **[Paper Presentation]** Anup Shakya, Vasile Rus and Deepak Venugopal. [Mastery Guided Non-parametric Clustering to Scale-up Strategy Prediction](#mastery-guided-non-parametric-clustering-to-scale-up-strategy-prediction)
    * Camera Ready [PDF](https://drive.google.com/file/d/1z_I9sPaRIgDVdeLh4bDRWTGQJtkkGJVL/view?usp=share_link)
* 15:20 - 15:30 **[Paper Presentation]** Tim Klausmann, Marius Köppel, Daniel Schunk and Isabell Zipperle. [Can machine learning solve the challenge of adaptive learning and the individualization of learning paths? A field experiment in an online learning platform](#can-machine-learning-solve-the-challenge-of-adaptive-learning-and-the-individualization-of-learning-paths-a-field-experiment-in-an-online-learning-platform)
    * Camera Ready [PDF](https://drive.google.com/file/d/1nmiiM-1FXVKlgfzBJ_Wi_GK1NNA-zCKP/view?usp=share_link)
* 15:30 - 15:40 **[Paper Presentation]** John Stamper, Bharat Gaind, Karun Thankachan, Huy Nguyen, Steven Moore. [Hierarchical Concept Map Generation from Course Data](#hierarchical-concept-map-generation-from-course-data)
    * Camera Ready [PDF](https://drive.google.com/file/d/1Jn2tISf-yWH1dE3EWlxfAFoOXd7sx26e/view?usp=share_link)
* 15:40 - 15:50 **[Paper Presentation]** Hyeondey Kim, Jinwoo Nam, Minjae Lee, Yun Jegal and Kyungwoo Song. [Leveraging Skill-to-Skill Supervision for Knowledge Tracing](#leveraging-skill-to-skill-supervision-for-knowledge-tracing)
    * Camera Ready [PDF](https://drive.google.com/file/d/1Q-Ji6glB7RWtKmPouCPPvyA_YhgMWfAV/view?usp=share_link)
* 15:50 - 16:00 **[Paper Presentation]** Alexander Jenkins, Imad Jaimoukha, Ljubisa Stankovic and Danilo Mandic. [Fair and Skill-diverse Student Group Formation via Constrained K-way Graph Partitioning](#fair-and-skill-diverse-student-group-formation-via-constrained-k-way-graph-partitioning)
    * Camera Ready [PDF]()
* 16:00 - 16:10 **[Paper Presentation]** Xiaofei Zhou, Hanjia Lyu, Jiebo Luo and Zhen Bai. [ML-SD Modeling: How Machine Learning Can Support Scientific Discovery Learning for K-12 STEM Education](#ml-sd-modeling-how-machine-learning-can-support-scientific-discovery-learning-for-k-12-stem-education)
    * Camera Ready [PDF]()
* 16:10 - 16:20 **[Paper Presentation]** Vishesh Kalvakurthi, Aparna Varde and John Jenq. [Hey Dona! Can you help me with student course registration?](#hey-dona-can-you-help-me-with-student-course-registration)
    * Camera Ready [PDF](https://drive.google.com/file/d/1NX_7B6DIBcSqtlKjuH_X3w7JPcVOr6yx/view?usp=share_link)
* 16:20 - 16:30 **[Paper Presentation]** Roberto Daza, Luis F. Gomez, Aythami Morales, Ruben Tolosana, Julian Fierrez, Ruth Cobos and Javier Ortega-Garcia. [MATT: Multimodal Attention Level Estimation for e-learning Platforms](#matt-multimodal-attention-level-estimation-for-e-learning-platforms)
    * Camera Ready [PDF](https://drive.google.com/file/d/1neJzRVSTqGDmHjNU9TUr5UsOfQNtawCh/view?usp=share_link)
* 16:30 - 16:40 **[Paper Presentation]** Jia Tracy Shen and Dongwon Lee. [Imputing Knowledge Tracing Data with Subject-Based Training via LSTM Variational Autoencoders](#imputing-knowledge-tracing-data-with-subject-based-training-via-lstm-variational-autoencoders)
    * Camera Ready [PDF](https://drive.google.com/file/d/1yA8p9WZI3mS1cwV3Mem-ox5JdoQGAc2H/view?usp=share_link)
* 16:40 - 16:50 **[Paper Presentation]** Levi Corallo and Aparna Varde. [Optical Character Recognition and Transcription of Berber Signs from Images in a Low-Resource Language Amazigh](#optical-character-recognition-and-transcription-of-berber-signs-from-images-in-a-low-resource-language-amazigh)
    * Camera Ready [PDF](https://drive.google.com/file/d/1QgY20sz3a-u_UQ7cygWyse9q6OmjJ1nS/view?usp=share_link)
* 16:50 - 17:00 **[Paper Presentation]** Frederik Baucks, Robin Schmucker and Laurenz Wiskott. [Tracing Changes in University Course Difficulty Using Item-Response Theory](#tracing-changes-in-university-course-difficulty-using-item-response-theory)
    * Camera Ready [PDF](https://drive.google.com/file/d/1LT7p6oSa1aXrcxsZhyWBLLhi_nKk3jM5/view?usp=share_link)


## Talk Details

> ### From Autonomy to Synergy: Envisioning Next Generation Human-AI Partnerships
> 
> <cite>Sidney K. D’Mello</cite>

{% highlight html %}
Abstract: I’ll describe a vision for AI, beyond a cold, autonomous agent working for humans to a socio-technical synergistic partnership with humans. I’ll highlight these ideas in the context of the NSF National AI Institute for Student AI Teaming (http://www.isat.ai), which brings together a geographically distributed team of researchers with K-12 partners to reframe the role of AI in education, moving from a facilitator of personalized one-on-one learning, to a social, collaborative partner which helps students and teachers work and learn more effectively, engagingly, and equitably. The talk will highlight insights and technologies at the intersection of the Institute’s three research strands: (1) foundational AI to understand and facilitate conversations; (2) orchestrating classroom interactions with AI; and (3) broadening participation with curriculum co-design. I’ll contextualize the research within our commitment to responsible innovation and polycultural approaches for developing ethical AI technologies.

Bio: Sidney D’Mello (PhD in Computer Science) is a Professor in the Institute of Cognitive Science and Department of Computer Science at the University of Colorado Boulder. He is interested in the dynamic interplay between cognition and emotion while individuals and groups engage in complex real-world activities. He applies insights gleaned from this basic research program to develop intelligent technologies that help people achieve to their fullest potential by coordinating what they think and feel with what they know and do. D’Mello has co-edited seven books and has published more than 300 journal papers, book chapters, and conference proceedings. His research has received 17 awards at international conferences and has been funded by numerous grants. D’Mello serves(d) as Associate Editor and on the Editorial Boards of 11 journals. He leads the NSF National Institute for Student-Agent Teaming (2020-2025), which aims to develop AI technologies to facilitate rich socio-collaborative learning experiences for all students.
{% endhighlight %}

<!-- * 14:30 - 14:40 Break -->
> ### Finding Similar Exercises in Retrieval Manner
> 
> <cite>Tongwen Huang, Xihua Li and Yi Chao</cite>

{% highlight html %}
Abstract: When students make a mistake in an exercise, they can consolidate it by “similar exercises” which have the same concepts, purposes and methods. Commonly, for a certain subject and study stage, the size of the exercise bank is in the range of millions to even tens of millions, how to find similar exercises for a given exercise becomes a crucial technical problem. Generally, we can assign a variety of explicit labels to the exercise, and then query through the labels, but the label annotation is time-consuming, laborious and costly, with limited precision and granularity, so it is not feasible. In practice, we define “similar exercises” as a retrieval process of finding a set of similar exercises based on recall, ranking and re-rank procedures, called the FSE problem (Finding similar exercises). In other papers, FSE focuses on ranking method, this paper will comprehensively introduce recall, ranking, re-rank, and define similar exercise more accurately. Furthermore, comprehensive representation of the semantic information of exercises was obtained through representation learning. In addition to the reasonable architecture, we also explore what kind of tasks are more conducive to the learning of exercise semantic information from pre-training and supervised learning. It is difficult to annotate similar exercises and the annotation consistency among experts is low. Therefore this paper also provides solutions to solve the problem of low-quality annotated data. Compared with other methods, this paper has obvious advantages in both architecture rationality and algorithm precision, which now serves the daily teaching of hundreds of schools.
{% endhighlight %}


> ### TQ-Net: Mixed Contrastive Representation Learning For Heterogeneous Test Questions
> 
> <cite>He Zhu, Xihua Li, Xuemin Zhao, Yunbo Cao and Shan Yu</cite>

{% highlight html %}
Abstract: Recently, more and more people study online for the convenience of access to massive learning materials (e.g. test questions/notes), thus accurately understanding learning materials became a crucial issue, which is essential for many educational applications. Previous studies focus on using language models to represent the question data. However, test questions (TQ) are usually heterogeneous and multi-modal, e.g., some of them may only contain text, while others half contain images with information beyond their literal description. In this context, both supervised and unsupervised methods are difficult to learn a fused representation of questions. Meanwhile, this problem cannot be solved by conventional methods such as image caption, as the images may contain information complementary rather than duplicate to the text. In this paper, we first improve previous text-only representation with a two-stage unsupervised instance level contrastive based pre-training method (MCL: Mixture Unsupervised Contrastive Learning). Then, TQ-Net was proposed to fuse the content of images to the representation of heterogeneous data. Finally, supervised contrastive learning was conducted on relevance prediction-related downstream tasks, which help the model to effectively learn the representation of questions. We conducted extensive experiments on question-based tasks on large-scale, real-world datasets, which demonstrated the effectiveness of TQ-Net and improve the precision of downstream applications (e.g. similar questions ↑2.02% and knowledge point prediction ↑7.20%). Our code will be available, and we will open-source a subset of our data to promote the development of relative studies.
{% endhighlight %}



> ### Inferring Actions and Joint Attention From Dual-view Classroom Videos
> 
> <cite>Fang Nan, Feng Tian, Yaozhi Wang, Qidong Liu, Yanze Wu, Jizhong Zhang, Huan Li, Haiping Zhu, Yuzhe Yao, Heng Zhang, Yaqiang Wu and Qinghua Zheng</cite>

{% highlight html %}
Abstract: There is a handful of pedagogical studies on the assessment of in-classroom teaching quality that indicate that recognizing student/teacher actions in the classroom and their non-contact interaction is necessary. Although modern classrooms are often equipped with dual-view cameras, i.e., forward and backward, which is a widely applied CCTV installation solution, existing recognition methods of recognizing student/teacher actions and understanding their attentions confront two challenges, (i) the combination explosion of Multi-Human and Multi-Object in Classroom (MHMOC) scenes and (ii) the dual-view separation between teacher-oriented and student-oriented videos. In this paper, we propose a novel framework to precisely recognize the actions and interactions between teachers and students in dual-view camera settings. Accordingly, a pose-feature-based graph neural network HOI detection and a joint attention detection are proposed to address the challenge. Our proposed method is comprehensively evaluated against various state-of-the-art methods, and the result shows that our method significantly outperforms the baselines with improvements of 22.4%,60.5% and 74.1% on the mAP, AUC and L2 distance, respectively. Especially, a prototype system based on our research is already deployed in real-world applications.
{% endhighlight %}


> ### Mastery Guided Non-parametric Clustering to Scale-up Strategy Prediction
> 
> <cite>Anup Shakya, Vasile Rus and Deepak Venugopal</cite>

{% highlight html %}
Abstract: Predicting the strategy (sequence of concepts) that a student is likely to use in problem-solving helps Adaptive Instructional Systems (AISs) better adapt themselves to different types of learners based on their learning abilities. This can lead to a more dynamic, engaging, and personalized experience for students. To scale up training a prediction model (such as LSTMs) over large-scale education datasets, we develop a non-parametric approach to cluster symmetric instances in the data. Specifically, we learn a representation based on Node2Vec that encodes symmetries over mastery or skill level since, to solve a problem, it is natural that a student’s strategy is likely to involve concepts in which they have gained mastery. Using this representation, we use DP-Means to group symmetric instances through a coarse-to-fine refinement of the clusters. We apply our model to learn strategies for Math learning from large-scale datasets from MATHia, a leading AIS for middle-school math learning. Our results illustrate that our approach can consistently achieve high accuracy using a small sample that is representative of the full dataset. Further, we show that this approach helps us learn strategies with high accuracy for students at different skill levels, i.e., leveraging symmetries improves fairness in the prediction model.
{% endhighlight %}


> ### Can machine learning solve the challenge of adaptive learning and the individualization of learning paths? A field experiment in an online learning platform
> 
> <cite>Tim Klausmann, Marius Köppel, Daniel Schunk and Isabell Zipperle</cite>

{% highlight html %}
Abstract: The individualization of learning contents with digital technologies promises large individual and social benefits. However, the optimal implementation of individualization remains an open question. To tackle this question we conduct a randomized controlled trial on a large digital self-learning platform. We develop an algorithm based on two convolutional neural networks that assigns tasks to 4,365 learners according to their learning paths. Learners are randomized into three groups: two treatment groups – a group-based adaptive treatment group and an individual adaptive treatment group and one control group. We analyze the difference between the three groups with respect to effort learners provide and their performance on the platform. We do not find differences between our groups. This results light on the multiple challenges associated with the individualization of learning paths.
{% endhighlight %}


> ### Hierarchical Concept Map Generation from Course Data
> 
> <cite>John Stamper, Bharat Gaind, Karun Thankachan, Huy Nguyen, Steven Moore</cite>

{% highlight html %}
Abstract: Concept maps are a core feature supporting the design, development, and improvement of online courses and educational technologies. Providing hierarchical ordering of the concepts allows for a more detailed understanding of course content by indicating pre- and post-requisite information. In this research, we implement an end-to-end domain-independent system to generate a concept map from digital texts that needs no additional data augmentation. We extract concepts from digital textbooks on the domains of precalculus, physics, computer networks, and economics. We engineer seven relevant features to identify prerequisite relationships between the concepts. These prerequisites are then used to generate and visualize a hierarchical concept map for each course. Our experiments show that the proposed methodology exceeds the existing baseline performance in existing domains including physics and computer networking, by up to 14.5%. Additionally, human evaluation identified four common errors between the prerequisites found through use of the concept maps. Our findings indicate that our methods, which require minimal data preprocessing, can be used to create more informative concept maps. These concept maps can be leveraged by students, instructors, and course designers to improve the learning process in a variety of domains.
{% endhighlight %}



> ### Leveraging Skill-to-Skill Supervision for Knowledge Tracing
> 
> <cite>Hyeondey Kim, Jinwoo Nam, Minjae Lee, Yun Jegal and Kyungwoo Song</cite>

{% highlight html %}
Abstract: Knowledge tracing plays a pivotal role in intelligent tutoring systems. This task aims to predict the probability of students answering correctly to specific questions. To do so, knowledge tracing systems should trace the knowledge state of the students by utilizing their problem-solving history and knowledge about the problems. Recent advances in knowledge tracing models have enabled better exploitation of problem solving history. However, knowledge about problems has not been studied, as well compared to students’ answering histories. Knowledge tracing algorithms that incorporate knowledge directly are important to settings with limited data or cold starts. Therefore, we consider the problem of utilizing skill-to-skill relation to knowledge tracing. In this work, we introduce expert labeled skill-to-skill relationships. Moreover, we also provide novel methods to construct a knowledge-tracing model to leverage human experts’ insight regarding relationships between skills. The results of an extensive experimental analysis show that our method outperformed a baseline Transformer model. Furthermore, we found that the extent of our model’s superiority was greater in situations with limited data, which allows a smooth cold start of our model.
{% endhighlight %}


> ### Fair and Skill-diverse Student Group Formation via Constrained K-way Graph Partitioning
> 
> <cite>Alexander Jenkins, Imad Jaimoukha, Ljubisa Stankovic and Danilo Mandic</cite>

{% highlight html %}
Abstract: 
{% endhighlight %}


> ### ML-SD Modeling: How Machine Learning Can Support Scientific Discovery Learning for K-12 STEM Education
> 
> <cite>Xiaofei Zhou, Hanjia Lyu, Jiebo Luo and Zhen Bai</cite>

{% highlight html %}
Abstract: 
{% endhighlight %}


> ### Hey Dona! Can you help me with student course registration?
> 
> <cite>Vishesh Kalvakurthi, Aparna Varde and John Jenq</cite>

{% highlight html %}
Abstract: In this paper. we present a demo of an intelligent personal agent Hey Dona or just Dona with virtual voice assistance in student course registration. It is a deployed project in the theme of AI for education. In this digital age with a myriad of smart devices, users often delegate tasks to agents. While pointing and clicking supersedes the erstwhile commandtyping, modern devices allow users to speak commands for agents to execute tasks, enhancing speed and convenience. In line with this progress, Dona is an intelligent agent catering to student needs by automated, voice-operated course registration, spanning a multitude of accents, entailing task planning optimization, with some language translation as needed. Dona accepts voice input by microphone (Bluetooth, wired microphone), converts human voice to computer understandable language, performs query processing as per user commands, connects with the Web to search for answers, models task dependencies, imbibes quality control, and conveys output by speaking to users as well as displaying text, thus enabling human-AI interaction by speech cum text. It is meant to work seamlessly on desktops, smartphones etc. and in indoor / outdoor settings. To the best of our knowledge, Dona is among the first of its kind as an intelligent personal agent for voice assistance in student course registration. Due to its ubiquitous access for educational needs, Dona directly impacts AI for education. It makes a broader impact on smart city characteristics of smart living and smart people due to its contributions to providing benefits for new ways of living and assisting 21st century education, respectively.
{% endhighlight %}


> ### MATT: Multimodal Attention Level Estimation for e-learning Platforms
> 
> <cite>Roberto Daza, Luis F. Gomez, Aythami Morales, Ruben Tolosana, Julian Fierrez, Ruth Cobos and Javier Ortega-Garcia</cite>

{% highlight html %}
Abstract: This work presents a new multimodal system for remote attention level estimation based on multimodal face analysis. Our multimodal approach uses different parameters and signals obtained from the behavior and physiological processes that have been related to modeling cognitive load such as faces gestures (e.g., blink rate, facial actions units) and user actions (e.g., head pose, distance to the camera). The multimodal system uses the following modules based on Convolutional Neural Networks (CNNs): Eye blink detection, head pose estimation, facial landmark detection, and facial expression features. First, we individually evaluate the proposed modules in the task of estimating the student’s attention level captured during online e-learning sessions. For that we trained binary classifiers (high or low attention) based on Support Vector Machines (SVM) for each module. Secondly, we find out to what extent multimodal score level fusion improves the attention level estimation. The mEBAL database is used in the experimental framework, a public multi-modal database for attention level estimation obtained in an e-learning environment that contains data from 38 users while conducting several e-learning tasks of variable difficulty (creating changes in student cognitive loads).
{% endhighlight %}


> ### Imputing Knowledge Tracing Data with Subject-Based Training via LSTM Variational Autoencoders
> 
> <cite>Jia Tracy Shen and Dongwon Lee</cite>

{% highlight html %}
Abstract: The issue of missing data poses a great challenge on boosting performance and application of deep learning models in the Knowledge Tracing (KT) problem. However, there has been the lack of understanding on the issue in the literature. In this work, to address this challenge, we adopt a subject-based training method to split and impute data by student IDs instead of row number splitting which we call nonsubject based training. The benefit of subject-based training can retain the complete sequence for each student and hence achieve efficient training. Further, we leverage two existing deep generative frameworks, namely variational Autoencoders (VAE) and Longitudinal Variational Autoencoders (LVAE) frameworks and build LSTM kernels into them to form LSTM-VAE and LSTM LVAE (noted as VAE and LVAE for simplicity) models to generate quality data. In LVAE, a Gaussian Process (GP) model is trained to disentangle the correlation between the subject (i.e., student) descriptor information (e.g., age, gender) and the latent space. The paper finally compare the model performance between training the original data and training the data imputed with generated data from non-subject based model VAE-NS and subjectbased training models (i.e., VAE and LVAE). We demonstrate that the generated data from LSTM-VAE and LSTMLVAE can boost the original model performance by about 50%. Moreover, the original model just needs 10% more student data to surpass the original performance if the prediction model is small and 50% more data if the prediction model is large with our proposed frameworks.
{% endhighlight %}



> ### Optical Character Recognition and Transcription of Berber Signs from Images in a Low-Resource Language Amazigh
> 
> <cite>Levi Corallo and Aparna Varde</cite>

{% highlight html %}
Abstract: The Berber, or “Amazigh” language family is a low-resource North African vernacular language spoken by the indigenous Berber ethnic group. It has its own unique alphabet “Tifinagh” used across Berber communities in Morocco, Algeria, and others. The Afroasiatic language Berber is spoken by 14 million people, yet lacks adequate representation in education, research, web applications etc. For instance, there is no option for “to or from Amazigh / Berber” on Google Translate, which hosts over 100 languages today. Consequently, we do not find specialized educational apps, L2 (2nd language learner) acquisition, automated language translation, and remote-access facilities enabled in Berber. Motivated by this background, we propose a supervised approach called DaToBS: for Detection and Transcription of Berber Signs. The DaToBS approach entails the automatic recognition and transcription of Tifinagh characters from signs in photographs of natural environments. This is achieved by self-creating a corpus of 1862 pre-processed character images; curating the corpus with human-guided annotation; and feeding it into an OCR model via the deployment of CNN for deep learning based on computer vision models. We deploy computer vision modeling (rather than language models) because there are pictorial symbols in this alphabet, this deployment being a novel aspect of our work. The DaToBS experimentation and analyses yield over 92% accuracy in our pilot research. To the best of our knowledge, ours is among the first few works in the automated transcription of Berber signs from roadside images with deep learning, yielding high accuracy. This can pave the way for developing pedagogical applications in the Berber language, thereby addressing an important goal of outreach to underrepresented communities via AI in education.
{% endhighlight %}



> ### Tracing Changes in University Course Difficulty Using Item-Response Theory
> 
> <cite>Frederik Baucks, Robin Schmucker and Laurenz Wiskott</cite>

{% highlight html %}
Abstract: Curriculum analytics (CA) studies educational program structure and student data to ensure the quality of courses inside a curriculum. Ensuring low variation in course difficulty over time is crucial to warrant equal treatment of individual student cohorts and consistent degree outcomes. Still, existing CA techniques (e.g., process mining/simulation and curriculum-based prediction) are unable to capture such temporal variations due to their central assumption of timeinvariant course behavior. In this paper, we introduce item response theory (IRT) as a new methodology to the CA domain to address the open problem of tracing changes in course difficulty over time. We show the suitability of IRT to capture variance in course performance data and assess the validity and reliability of IRT-based difficulty estimates. Using data from 664 CS Bachelor students, we show how IRT can yield valuable insights by revealing variations in course difficulty over multiple years. Furthermore, we observe a systematic shift in course difficulty during the COVID-19 pandemic.
{% endhighlight %}


## Workshop Description


In this workshop, we invited AIED enthusiasts from all around the world through the following **three different channels**:

* First, we invited established researchers in the AIED community to give a <span style="color:red">broad talk</span> that (1) describes a vision for bridging AIED communities; (2) summarizes a well-developed AIED research area; or (3) presents promising ideas and visions for new AIED research directions. 

* Second, we called for <span style="color:red">regular workshop paper submissions</span> and <span style="color:red">cross-submissions</span> (papers that have appeared in or submitted to alternative venues) related to a broad range of AI domains for education. 

* Third, we hosted a <span style="color:red">global challenge</span> on Codalab for a fair comparison of state-of-the-art **Knowledge Tracing** models and invited technical reports from winning teams. 



### Regular Workshop Paper Submission

We invite high-quality paper submissions of theoretical and experimental nature on the broad AIED topics. The workshop solicits **4-7 pages double-blind** paper submissions from participants. Submissions of the following flavors will be sought: (1) research ideas, (2) case studies (or deployed projects), (3) review papers, (4) best practice papers, and (5) lessons learned. The format is the standard double-column [AAAI Proceedings Style](https://www.aaai.org/Publications/Templates/AnonymousSubmission23.zip). All submissions will be peer-reviewed. Some will be selected for spotlight talks, and some for the poster session.



### Cross-submissions 

In addition to previously unpublished work, we invite papers on relevant topics which have appeared in alternative venues (such as other ML or AIED conferences). Accepted cross-submissions will be presented as 15-min oral presentations, with an indication of the original venue. Selection of cross-submissions will be determined solely by the organizing committee. There will be no regular paper reviews for cross submissions. Please use the **original paper format** when submitting to the cross-submission track.


### Submission Website

Submission website: [https://easychair.org/conferences/?conf=aaai2023ai4edu](https://easychair.org/conferences/?conf=aaai2023ai4edu). The submission AUTHOR KIT can be found at [https://www.aaai.org/Publications/Templates/AnonymousSubmission23.zip](https://www.aaai.org/Publications/Templates/AnonymousSubmission23.zip).


### Global Knowledge Tracing Challenge

In this competition, we would like to call for researchers and practitioners worldwide to investigate the opportunities of **improving the student assessment performance via knowledge tracing approaches with rich side information**. 


![A graphical illustration of knowledge tracing]({{site.baseurl}}/images/aaai2023competition_kt.jpg)


The details of this competition can be found at [http://ai4ed.cc/competitions/aaai2023competition](http://ai4ed.cc/competitions/aaai2023competition).

## Important Dates

* **Nov 27, 2022**: Workshop paper submission due AOE
* ~~**Dec 25, 2022**~~ **<span style="color:red">Jan 01, 2023</span>**: Notifications of acceptance
* **Jan 5, 2023**: Deadline of the camera-ready final paper submission
* **Feb 13, 2023**: Workshop Date 



## Organizers

<!-- ![Beautiful place]({{site.baseurl}}/images/aaai2022_workshop_organizers.jpg) -->

* **Zitao Liu** TAL Education Group & Jinan University, China
* **Weiqi Luo** Guangdong Institute of Smart Education, Jinan University, China
* **Shaghayegh (Sherry) Sahebi** University at Albany – SUNY, USA
* **Lu Yu** Beijing Normal University, China
* **Richard Tong** IEEE AI Standard Committee & Squirrel AI Learning, USA
* **Jiahao Chen** TAL Education Group, China
* **Qiongqiong Liu** TAL Education Group, China